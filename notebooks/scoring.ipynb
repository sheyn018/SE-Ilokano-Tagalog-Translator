{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORING (BLEU, ROUGE, TER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu, modified_precision, corpus_bleu\n",
    "from functools import reduce\n",
    "from statistics import mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, filename):\n",
    "    smoothing = SmoothingFunction()\n",
    "    penalties_list = []\n",
    "    avg_precisions_list = []\n",
    "    bleu_scores_list = []\n",
    "    csv_frame = pd.DataFrame(columns=[\"INPUT\", \"REFERENCE\", \"MACHINE_TRANSLATION\", \"BREVITY_PENALTY\", \"GEOMETRIC_AVE\", \"BLEU_SCORE\"])\n",
    "\n",
    "    for index, target_op in enumerate(tgt_op_list):\n",
    "        system_op = sys_op_list[index]\n",
    "        source_text = src_text_list[index]\n",
    "\n",
    "        target_senlen = len(target_op.split())\n",
    "        system_senlen = len(system_op.split())\n",
    "\n",
    "        if system_senlen == target_senlen:\n",
    "            penalty = 1.0\n",
    "        else:\n",
    "            penalty = min(1, (target_senlen + 1) / (system_senlen + 1))\n",
    "\n",
    "        penalties_list.append(penalty)\n",
    "\n",
    "        score = sentence_bleu([target_op.split()], system_op.split(), smoothing_function=smoothing.method4)\n",
    "        avg_precision = (score ** (1 / 4))\n",
    "        avg_precisions_list.append(avg_precision)\n",
    "\n",
    "        bleu_score = penalty * avg_precision\n",
    "        bleu_scores_list.append(bleu_score)\n",
    "\n",
    "        csv_frame = csv_frame.append({\"INPUT\": source_text, \"REFERENCE\": target_op, \"MACHINE_TRANSLATION\": system_op, \"BREVITY_PENALTY\": penalty, \"GEOMETRIC_AVE\": avg_precision, \"BLEU_SCORE\": bleu_score}, ignore_index=True)\n",
    "        \n",
    "    average_bleu_score = mean(bleu_scores_list)\n",
    "    csv_frame = csv_frame.append({\"AVERAGE_BLEU_SCORE\": average_bleu_score}, ignore_index=True)\n",
    "    csv_frame.to_csv(filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, filename):\n",
    "    correct_candidate_words = 0\n",
    "    total_reference_words = 0\n",
    "    correct_reference_words = 0\n",
    "    total_candidate_words = 0\n",
    "    recall_sum = 0\n",
    "    precision_sum = 0\n",
    "    f1_sum = 0\n",
    "    csv_frame = pd.DataFrame(columns=[\"INPUT\", \"REFERENCE\", \"MACHINE_TRANSLATION\", \"CANDIDATE_IN_REF\", \"TOTAL_IN_REF\", \"RECALL\", \"REF_IN_CANDIDATE\", \"TOTAL_IN_CANDIDATE\", \"PRECISION\", \"F1_SCORE\"])\n",
    "\n",
    "    for index, target_op in enumerate(tgt_op_list):\n",
    "        source_text = src_text_list[index]\n",
    "        system_op = sys_op_list[index]\n",
    "\n",
    "        reference_words = target_op.split()\n",
    "        candidate_words = system_op.split()\n",
    "\n",
    "        # Increment the number of correctly identified candidate words in the reference\n",
    "        correct_candidate_words = len(set(candidate_words) & set(reference_words))\n",
    "        \n",
    "        # Increment the total number of words in the reference\n",
    "        total_reference_words = len(reference_words)\n",
    "        \n",
    "        # Calculate the number of correctly identified reference words in the candidate\n",
    "        correct_reference_words = len(set(candidate_words) & set(reference_words))\n",
    "        \n",
    "        # Calculate the total number of words in the candidate\n",
    "        total_candidate_words = len(candidate_words)\n",
    "\n",
    "        # Calculate the recall score\n",
    "        recall = correct_candidate_words / total_reference_words    \n",
    "        \n",
    "        # Calculate the precision score\n",
    "        precision = correct_reference_words / total_candidate_words\n",
    "        \n",
    "        #Calculate F1 Score\n",
    "        if precision == 0 or recall == 0:\n",
    "            f1_score = 0\n",
    "        else:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        recall_sum += recall\n",
    "        precision_sum += precision\n",
    "        f1_sum += f1_score\n",
    "\n",
    "        csv_frame = csv_frame.append({\"INPUT\": source_text, \"REFERENCE\": target_op, \"MACHINE_TRANSLATION\": system_op, \"CANDIDATE_IN_REF\": correct_candidate_words, \"TOTAL_IN_REF\": total_reference_words, \"RECALL\": recall, \"REF_IN_CANDIDATE\": correct_reference_words, \"TOTAL_IN_CANDIDATE\": total_candidate_words, \"PRECISION\": precision, \"F1_SCORE\": f1_score}, ignore_index=True)\n",
    "        \n",
    "    # Calculate the average recall, precision, and F1 scores\n",
    "    average_recall = recall_sum / len(src_text_list)\n",
    "    average_precision = precision_sum / len(src_text_list)\n",
    "    average_f1 = f1_sum / len(src_text_list)\n",
    "    csv_frame = csv_frame.append({\"AVERAGE_RECALL\": average_recall, \"AVERAGE_PRECISION\": average_precision, \"AVERAGE_F1\": average_f1}, ignore_index=True)\n",
    "    csv_frame.to_csv(filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ter_score(src_text_list, tgt_op_list, sys_op_list, filename):\n",
    "    target = [['']]\n",
    "    system = ['']\n",
    "    ter_scores_list = []\n",
    "    csv_frame = pd.DataFrame(columns=[\"INPUT\", \"REFERENCE\", \"MACHINE_TRANSLATION\", \"NUM_OF_EDITS\", \"NUM_OF_REF\", \"TER_SCORE\"])\n",
    "\n",
    "    for index, target_op in enumerate(tgt_op_list):\n",
    "        system_op = sys_op_list[index]\n",
    "        source_text = src_text_list[index]\n",
    "\n",
    "        reference_words = target_op.split()\n",
    "        candidate_words = system_op.split()\n",
    "\n",
    "        correct_candidate_words = len(set(candidate_words) & set(reference_words))\n",
    "        total_reference_words = len(reference_words)\n",
    "        num_of_edits = total_reference_words - correct_candidate_words\n",
    "\n",
    "        system[0] = system_op\n",
    "        target[0] = [target_op]\n",
    "        \n",
    "        ter_score = num_of_edits/total_reference_words\n",
    "\n",
    "        ter_scores_list.append(ter_score)\n",
    "        \n",
    "        csv_frame = csv_frame.append({\"INPUT\": source_text, \"REFERENCE\": target_op, \"MACHINE_TRANSLATION\": system_op, \"NUM_OF_EDITS\": num_of_edits, \"NUM_OF_REF\": total_reference_words, \"TER_SCORE\": ter_score}, ignore_index=True)\n",
    "        \n",
    "    average_ter_score = mean(ter_scores_list)\n",
    "    csv_frame = csv_frame.append({\"AVERAGE_TER_SCORE\": average_ter_score}, ignore_index=True)\n",
    "    csv_frame.to_csv(filename, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYBRID SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Ilokano to Tagalog\n",
    "dict_il_tl_result = pd.read_json('../src/json data/Ilokano to Tagalog/Hybrid Translator/dict_il-tl_op_ex.json')\n",
    "src_text_list = dict_il_tl_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_il_tl_result['Target Output'].tolist()\n",
    "sys_op_list = dict_il_tl_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/train/train_bleu_il-tl.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/train/train_rouge_il-tl.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/train/train_ter_il-tl.csv')\n",
    "\n",
    "# Training Data Tagalog to Ilokano \n",
    "dict_tl_il_result = pd.read_json('../src/json data/Tagalog to Ilokano/Hybrid Translator/dict_tl-il_op_ex.json')\n",
    "src_text_list = dict_tl_il_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_tl_il_result['Target Output'].tolist()\n",
    "sys_op_list = dict_tl_il_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/train/train_bleu_tl-il.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/train/train_rouge_tl-il.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/train/train_ter_tl-il.csv')\n",
    "\n",
    "# Testing Data Ilokano to Tagalog\n",
    "dict_il_tl_result = pd.read_json('../src/json data/Ilokano to Tagalog/Hybrid Translator/dict_il-tl_test.json')\n",
    "src_text_list = dict_il_tl_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_il_tl_result['Target Output'].tolist()\n",
    "sys_op_list = dict_il_tl_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/test/test_bleu_il-tl.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/test/test_rouge_il-tl.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Hybrid Translator/test/test_ter_il-tl.csv')\n",
    "\n",
    "# Testing Data Tagalog to Ilokano\n",
    "dict_tl_il_result = pd.read_json('../src/json data/Tagalog to Ilokano/Hybrid Translator/dict_tl-il_test.json')\n",
    "src_text_list = dict_tl_il_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_tl_il_result['Target Output'].tolist()\n",
    "sys_op_list = dict_tl_il_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/test/test_bleu_tl-il.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/test/test_rouge_tl-il.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Hybrid Translator/test/test_ter_tl-il.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STANDARD SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Ilokano to Tagalog\n",
    "dict_il_tl_result = pd.read_json('../src/json data/Ilokano to Tagalog/Standard Translator/dict_il_tl_result.json')\n",
    "src_text_list = dict_il_tl_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_il_tl_result['Target Output'].tolist()\n",
    "sys_op_list = dict_il_tl_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/train/train_bleu_il-tl.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/train/train_rouge_il-tl.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/train/train_ter_il-tl.csv')\n",
    "\n",
    "# Trainig Data Tagalog to Ilokano\n",
    "dict_tl_il_result = pd.read_json('../src/json data/Tagalog to Ilokano/Standard Translator/dict_tl_il_result.json')\n",
    "src_text_list = dict_tl_il_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_tl_il_result['Target Output'].tolist()\n",
    "sys_op_list = dict_tl_il_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/train/train_bleu_tl-il.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/train/train_rouge_tl-il.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/train/train_ter_tl-il.csv')\n",
    "\n",
    "# Testing Data Ilokano to Tagalog\n",
    "dict_il_tl_result = pd.read_json('../src/json data/Ilokano to Tagalog/Standard Translator/dict_il_tl_test.json')\n",
    "src_text_list = dict_il_tl_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_il_tl_result['Target Output'].tolist()\n",
    "sys_op_list = dict_il_tl_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/test/test_bleu_il-tl.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/test/test_rouge_il-tl.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Ilokano to Tagalog/Standard Translator/test/test_ter_il-tl.csv')\n",
    "\n",
    "# Testing Data Tagalog to Ilokano\n",
    "dict_tl_il_result = pd.read_json('../src/json data/Tagalog to Ilokano/Standard Translator/dict_tl_il_test.json')\n",
    "src_text_list = dict_tl_il_result['Source Text'].tolist()\n",
    "tgt_op_list = dict_tl_il_result['Target Output'].tolist()\n",
    "sys_op_list = dict_tl_il_result['System Output'].tolist()\n",
    "compute_bleu_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/test/test_bleu_tl-il.csv')\n",
    "compute_rouge_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/test/test_rouge_tl-il.csv')\n",
    "compute_ter_score(src_text_list, tgt_op_list, sys_op_list, '../src/scores/Tagalog to Ilokano/Standard Translator/test/test_ter_tl-il.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1d54cc6ba22d92170a9f9c24d6077688435e22a85a4273e6fe4e4e6bdebfd02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
