{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Machine Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.tl_il.rule_based_tl import dict_tl, remove_punct, tokenize, tag\n",
    "from module.tl_il.doc_trans_tl import combine_tokens\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tagalog Structure</th>\n",
       "      <th>Ilokano Structure</th>\n",
       "      <th>Ilokano Structure Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SW]</td>\n",
       "      <td>[[SW]]</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[VB, DT, NN]</td>\n",
       "      <td>[[DT, VB, DT, NN], [DT, NN, VB], [VB, DT, NN],...</td>\n",
       "      <td>[99, 358, 857, 40, 72, 61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[DT, NN]</td>\n",
       "      <td>[[DT, NN]]</td>\n",
       "      <td>[5041]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DT, VB, DT, NN]</td>\n",
       "      <td>[[DT, NN, VB], [DT, VB, DT, NN], [VB, DT, NN],...</td>\n",
       "      <td>[62, 177, 68, 15, 22, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[CC]</td>\n",
       "      <td>[[CC], [CC, RB]]</td>\n",
       "      <td>[3704, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tagalog Structure                                  Ilokano Structure  \\\n",
       "0              [SW]                                             [[SW]]   \n",
       "1      [VB, DT, NN]  [[DT, VB, DT, NN], [DT, NN, VB], [VB, DT, NN],...   \n",
       "2          [DT, NN]                                         [[DT, NN]]   \n",
       "3  [DT, VB, DT, NN]  [[DT, NN, VB], [DT, VB, DT, NN], [VB, DT, NN],...   \n",
       "4              [CC]                                   [[CC], [CC, RB]]   \n",
       "\n",
       "      Ilokano Structure Count  \n",
       "0                         [8]  \n",
       "1  [99, 358, 857, 40, 72, 61]  \n",
       "2                      [5041]  \n",
       "3   [62, 177, 68, 15, 22, 18]  \n",
       "4                   [3704, 3]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_tl_il_lm = pd.read_json('src/json data/Tagalog to Ilokano/Example-Based/Language Model/dict_tl_il_lang_mod.json')\n",
    "\n",
    "dict_tl_il_lm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the Structure Columns in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_struct = dict_tl_il_lm['Tagalog Structure'].tolist()\n",
    "il_struct = dict_tl_il_lm['Ilokano Structure'].tolist()\n",
    "il_struct_count = dict_tl_il_lm['Ilokano Structure Count'].tolist()\n",
    "\n",
    "# print(il_struct_count[0].index(max(il_struct_count[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the SMT columns in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    TF-IDF\n",
    "\"\"\"\n",
    "vb_tl_tf_idf_list = dict_tl.dict_vb['Tagalog Verb TF-IDF'].tolist()\n",
    "nn_tl_tf_idf_list = dict_tl.dict_nn['Tagalog Noun TF-IDF'].tolist()\n",
    "jj_tl_tf_idf_list = dict_tl.dict_jj['Tagalog Adjective TF-IDF'].tolist()\n",
    "rb_tl_tf_idf_list = dict_tl.dict_rb['Tagalog Adverb TF-IDF'].tolist()\n",
    "cc_tl_tf_idf_list = dict_tl.dict_cc['Tagalog Conjunction TF-IDF'].tolist()\n",
    "pr_tl_tf_idf_list = dict_tl.dict_pr['Tagalog Preposition TF-IDF'].tolist()\n",
    "dt_tl_tf_idf_list = dict_tl.dict_dt['Tagalog Determiner TF-IDF'].tolist()\n",
    "\n",
    "\"\"\"\n",
    "    Count Vectors\n",
    "\"\"\"\n",
    "vb_il_tf_cnt_list = dict_tl.dict_vb['Ilokano Verb Count'].tolist()\n",
    "nn_il_tf_cnt_list = dict_tl.dict_nn['Ilokano Noun Count'].tolist()\n",
    "jj_il_tf_cnt_list = dict_tl.dict_jj['Ilokano Adjective Count'].tolist()\n",
    "rb_il_tf_cnt_list = dict_tl.dict_rb['Ilokano Adverb Count'].tolist()\n",
    "cc_il_tf_cnt_list = dict_tl.dict_cc['Ilokano Conjunction Count'].tolist()\n",
    "pr_il_tf_cnt_list = dict_tl.dict_pr['Ilokano Preposition Count'].tolist()\n",
    "dt_il_tf_cnt_list = dict_tl.dict_dt['Ilokano Determiner Count'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchial Dependence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the SMT values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_tl(sen_poss_list, dict_source, not_in_sw, not_in_vb, not_in_nn, not_in_jj, not_in_rb, not_in_cc, not_in_pr, not_in_dt, not_tagged, sum_tf_idf_tl_list):\n",
    "    sp_index = 0 # sentence POS index\n",
    "    \n",
    "    for sen_poss in sen_poss_list:\n",
    "        # loop for getting the pos structure of every sentence\n",
    "        \"\"\"\n",
    "        sen_poss is a list of POS of a sentence\n",
    "        eg. ['VB', 'DT', 'NN', 'DT', 'NN']\n",
    "        \"\"\"\n",
    "        sen_translation = []\n",
    "        \n",
    "        sum_tf_idf_tl = 0\n",
    "        wp_index = 0 # word POS index\n",
    "        \n",
    "        for word_pos in sen_poss:\n",
    "            word = dict_source['Tokenized'][sp_index][wp_index]\n",
    "            # gets the word in every sentence\n",
    "            \n",
    "            # Matching Conditions    \n",
    "            # 1. SW\n",
    "            if word_pos == 'SW':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'SW'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.sw_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of single words\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.sw_tl_list.index(word)\n",
    "                    \n",
    "                else:\n",
    "                    not_in_sw.append(word) # for debugging purposes\n",
    "                                \n",
    "            # 2. SW\n",
    "            elif word_pos == 'VB':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'VB'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.vb_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of verbs\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.vb_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += vb_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_vb.append(word) # for debugging purposes\n",
    "            \n",
    "            # 3. NN\n",
    "            elif word_pos == 'NN':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'NN'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.nn_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.nn_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += nn_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_nn.append(word) # for debugging purposes\n",
    "            \n",
    "            # 4. JJ\n",
    "            elif word_pos == 'JJ':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'JJ'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.jj_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.jj_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += jj_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_jj.append(word) # for debugging purposes\n",
    "            \n",
    "            # 5. RB\n",
    "            elif word_pos == 'RB':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'RB'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.rb_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.rb_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += rb_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_rb.append(word) # for debugging purposes\n",
    "                    \n",
    "            # 6. CC\n",
    "            elif word_pos == 'CC':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'CC'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.cc_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.cc_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += cc_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_cc.append(word) # for debugging purposes\n",
    "                    \n",
    "            # 7. PR\n",
    "            elif word_pos == 'PR':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'CC'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.pr_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.pr_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += pr_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_pr.append(word) # for debugging purposes\n",
    "            \n",
    "             # 8. DT\n",
    "            elif word_pos == 'DT':\n",
    "                \"\"\"\n",
    "                if the POS of the word is 'DT'\n",
    "                \"\"\"\n",
    "                if word in dict_tl.dt_tl_list:\n",
    "                    \"\"\"\n",
    "                    if the word is in the Tagalog list of nouns\n",
    "                    \"\"\"\n",
    "                    temp_index = dict_tl.dt_tl_list.index(word)\n",
    "                    sum_tf_idf_tl += dt_tl_tf_idf_list[temp_index]\n",
    "                else:\n",
    "                    not_in_dt.append(word) # for debugging purposes\n",
    "            \n",
    "            else:\n",
    "                not_tagged.append(word) # for debugging purposes\n",
    "                \n",
    "            wp_index += 1\n",
    "        \n",
    "        sum_tf_idf_tl_list.append(round(sum_tf_idf_tl, 5))\n",
    "        sp_index += 1\n",
    "        \n",
    "    return sum_tf_idf_tl_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating with SMT translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_lm(ngram_data):\n",
    "    trans_ngram_data = []\n",
    "    for ngram_sen in ngram_data:\n",
    "        trans_ngram_sen = []\n",
    "        \n",
    "        for ngram in ngram_sen:\n",
    "            if ngram in tl_struct:\n",
    "                temp_index = tl_struct.index(ngram)\n",
    "                max_count = max(il_struct_count[temp_index])\n",
    "                trans_index = il_struct_count[temp_index].index(max_count)\n",
    "                trans_ngram = il_struct[temp_index][trans_index]\n",
    "                trans_ngram_sen.append(trans_ngram)\n",
    "            else:\n",
    "                trans_ngram_sen.append(ngram)\n",
    "                \n",
    "            # np_index += 1\n",
    "        \n",
    "        trans_ngram_data.append(trans_ngram_sen)\n",
    "        \n",
    "    return trans_ngram_data\n",
    "# end of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_phrases = pd.read_csv('src/csv data/f_phrases.csv')\n",
    "il_phrases = f_phrases['Ilokano'].to_list()\n",
    "il_phrases = [remove_punct(word) for word in il_phrases]\n",
    "il_phrases = [tokenize(word) for word in il_phrases]\n",
    "\n",
    "tl_phrases = f_phrases['Tagalog'].to_list()\n",
    "tl_phrases = [remove_punct(word) for word in tl_phrases]\n",
    "tl_phrases = [tokenize(word) for word in tl_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inFPhrases(word, word2, word3, word4, word5, word6, word7, tl_phrases):\n",
    "    inFPhrases = False\n",
    "    tl_phrase = []\n",
    "    w_used = 0\n",
    "    for phrase in tl_phrases:\n",
    "        length = len(phrase)\n",
    "        if length == 7:\n",
    "            if word == phrase[0] and word2 == phrase[1] and word3 == phrase[2] and word4 == phrase[3] and word5 == phrase[4] and word6 == phrase[5] and word7 == phrase[6]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 7\n",
    "                break\n",
    "        if length == 6:\n",
    "            if word == phrase[0] and word2 == phrase[1] and word3 == phrase[2] and word4 == phrase[3] and word5 == phrase[4] and word6 == phrase[5]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 6\n",
    "                break\n",
    "        if length == 5:\n",
    "            if word == phrase[0] and word2 == phrase[1] and word3 == phrase[2] and word4 == phrase[3] and word5 == phrase[4]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 5\n",
    "                break\n",
    "        if length == 4:\n",
    "            if word == phrase[0] and word2 == phrase[1] and word3 == phrase[2] and word4 == phrase[3]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 4\n",
    "                break\n",
    "        if length == 3:\n",
    "            if word == phrase[0] and word2 == phrase[1] and word3 == phrase[2]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 3\n",
    "                break\n",
    "        if length == 2:\n",
    "            if word == phrase[0] and word2 == phrase[1]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 2\n",
    "                break\n",
    "        if length == 1:\n",
    "            if word == phrase[0]:\n",
    "                inFPhrases = True\n",
    "                tl_phrase = phrase\n",
    "                w_used = 1\n",
    "                break \n",
    "                \n",
    "    return inFPhrases, tl_phrase, w_used\n",
    "# end of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.smt import encapsulate, ngram_var\n",
    "\n",
    "def translate_smt(sen_poss_list, dict_source):\n",
    "    not_in_sw = []\n",
    "    not_in_vb = []\n",
    "    not_in_nn = []\n",
    "    not_in_jj = []\n",
    "    not_in_rb = []\n",
    "    not_in_cc = []\n",
    "    not_in_pr = []\n",
    "    not_in_dt = []\n",
    "    not_tagged = []\n",
    "    sum_tf_idf_tl_list = []\n",
    "\n",
    "    sum_tf_idf_tl_list = get_sum_tl(sen_poss_list, dict_source, not_in_sw, not_in_vb, not_in_nn, not_in_jj, not_in_rb, not_in_cc, not_in_pr, not_in_dt, not_tagged, sum_tf_idf_tl_list)\n",
    "    \n",
    "    encapsulate(sen_poss_list, ngram_var.fourgram_list, ngram_var.trigram_list, ngram_var.bigram_list, ngram_var.unigram_list, ngram_var.ngram_list, ngram_var.notencap_list, ngram_var.fourgram_count_sen, ngram_var.trigram_count_sen, ngram_var.bigram_count_sen, ngram_var.unigram_count_sen, ngram_var.notencap_count_sen)\n",
    "    \n",
    "    ngram_data = ngram_var.ngram_list\n",
    "    \n",
    "    trans_ngram_data = trans_lm(ngram_data)\n",
    "    \n",
    "    sp_index = 0 # sentence POS index\n",
    "    sen_translation_list = []\n",
    "    \n",
    "    for sen_poss in sen_poss_list:\n",
    "        # loop for getting the pos structure of every sentence\n",
    "        \"\"\"\n",
    "        sen_poss is a list of POS of a sentence\n",
    "        eg. ['VB', 'DT', 'NN', 'DT', 'NN']\n",
    "        \"\"\"\n",
    "        sen_translation = []\n",
    "        wp_index = 0\n",
    "        cur_wp_index = 0\n",
    "        \n",
    "        for word_pos in sen_poss:\n",
    "            if wp_index == cur_wp_index:\n",
    "                word = dict_source['Tokenized'][sp_index][wp_index]\n",
    "                # gets the word in every sentence\n",
    "                \n",
    "                try: \n",
    "                    word2 = dict_source['Tokenized'][sp_index][wp_index+1]\n",
    "                except:\n",
    "                    word2 = None\n",
    "                try:\n",
    "                    word3 = dict_source['Tokenized'][sp_index][wp_index+2]\n",
    "                except:\n",
    "                    word3 = None\n",
    "                try:\n",
    "                    word4 = dict_source['Tokenized'][sp_index][wp_index+3]\n",
    "                except:\n",
    "                    word4 = None\n",
    "                try:\n",
    "                    word5 = dict_source['Tokenized'][sp_index][wp_index+4]\n",
    "                except:\n",
    "                    word5 = None\n",
    "                try:\n",
    "                    word6 = dict_source['Tokenized'][sp_index][wp_index+5]\n",
    "                except:\n",
    "                    word6 = None\n",
    "                try:\n",
    "                    word7 = dict_source['Tokenized'][sp_index][wp_index+6]\n",
    "                except:\n",
    "                    word7 = None\n",
    "                \n",
    "                ans = inFPhrases(word, word2, word3, word4, word5, word6, word7, tl_phrases)\n",
    "                inFPDict = ans[0]\n",
    "                tl_phrase = ans[1]\n",
    "                w_used = ans[2]                \n",
    "                \n",
    "                if inFPDict:\n",
    "                    \"\"\"\n",
    "                    if the word is in the list of Tagalog phrases\n",
    "                    \"\"\"\n",
    "                    p_index = tl_phrases.index(tl_phrase)\n",
    "                    il_phrase = il_phrases[p_index]\n",
    "                    for il_word in il_phrase:\n",
    "                        sen_translation.append(il_word)\n",
    "                    cur_wp_index = wp_index + w_used\n",
    "                    \n",
    "                else:\n",
    "                    cur_wp_index = wp_index + 1\n",
    "                    \n",
    "                    # Matching Conditions    \n",
    "                    # 1. SW\n",
    "                    if word_pos == 'SW':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'SW'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.sw_tl_list:\n",
    "                            temp_index = dict_tl.sw_tl_list.index(word)\n",
    "                            if dict_tl.sw_il_list[temp_index][0] == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(dict_tl.sw_il_list[temp_index][0])\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                    \n",
    "                    # 2. VB\n",
    "                    elif word_pos == 'VB':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'VB'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.vb_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of verbs\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.vb_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.vb_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.vb_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.vb_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                    \n",
    "                    # 3. NN\n",
    "                    elif word_pos == 'NN':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'NN'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.nn_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of noun\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.nn_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.nn_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.nn_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.nn_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                    \n",
    "                    # 4. JJ\n",
    "                    elif word_pos == 'JJ':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'JJ'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.jj_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of adjectives\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.jj_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.jj_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.jj_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.jj_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                            \n",
    "                    # 5. RB\n",
    "                    elif word_pos == 'RB':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'RB'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.rb_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of adverbs\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.rb_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.rb_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.rb_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.rb_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                    \n",
    "                    # 6. CC\n",
    "                    elif word_pos == 'CC':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'CC'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.cc_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of conjunctions\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.cc_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.cc_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.cc_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.cc_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                            \n",
    "                    # 7. PR\n",
    "                    elif word_pos == 'PR':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'PR'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.pr_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of prepositions\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.pr_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.pr_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.pr_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.pr_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                            \n",
    "                    # 8. DT\n",
    "                    elif word_pos == 'DT':\n",
    "                        \"\"\"\n",
    "                        if the POS of the word is 'DT'\n",
    "                        \"\"\"\n",
    "                        if word in dict_tl.dt_tl_list:\n",
    "                            \"\"\"\n",
    "                            if the word is in the Tagalog list of determiners\n",
    "                            \"\"\"\n",
    "                            tl_index = dict_tl.dt_tl_list.index(word)\n",
    "                            max_tlidf = max(dict_tl.dt_tfidf_il_list[tl_index])\n",
    "                            il_index = dict_tl.dt_tfidf_il_list[tl_index].index(max_tlidf)\n",
    "                            il_word = dict_tl.dt_il_list[tl_index][il_index]\n",
    "                            \n",
    "                            if il_word == 'None':\n",
    "                                sen_translation.append(word)\n",
    "                            else:\n",
    "                                sen_translation.append(il_word)\n",
    "                        else:\n",
    "                            sen_translation.append(word)\n",
    "                    \n",
    "                    else:\n",
    "                        sen_translation.append(word)\n",
    "\n",
    "            wp_index += 1\n",
    "        sp_index += 1\n",
    "        sen_translation_list.append(sen_translation)\n",
    "    \n",
    "    return sen_translation_list\n",
    "# end of function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and processing the Source Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_smt_nb(test_doc, target_op):\n",
    "    parsed_source = test_doc.split(\"\\n\")\n",
    "\n",
    "    cleaned_source = [remove_punct(word) for word in parsed_source]\n",
    "    toklenized_source = [tokenize(word) for word in cleaned_source]\n",
    "    dict_source = pd.DataFrame({'Tokenized': toklenized_source})\n",
    "    pos_sen_list = tag(dict_source['Tokenized'])\n",
    "\n",
    "    dict_source['POS'] = pos_sen_list\n",
    "    sen_translation_list = translate_smt(dict_source['POS'], dict_source)\n",
    "    temp_sen_list = combine_tokens(sen_translation_list)\n",
    "\n",
    "    # Dictionary of the system output and the expected output and their scores\n",
    "    dict_op_ex = pd.DataFrame({'Source Text':  cleaned_source})\n",
    "\n",
    "    parsed_expected_op = target_op.split(\"\\n\")\n",
    "    cleaned_expected_op = [remove_punct(word) for word in parsed_expected_op]\n",
    "    toklenized_expected_op = [tokenize(word) for word in cleaned_expected_op]\n",
    "    combine_tokens_expected_op = combine_tokens(toklenized_expected_op)\n",
    "    dict_op_ex['Target Output'] = combine_tokens_expected_op\n",
    "    dict_op_ex['System Output'] = temp_sen_list\n",
    "\n",
    "    dict_op_ex.head()\n",
    "    \n",
    "    return dict_op_ex\n",
    "# end of function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file\n",
    "# test_doc = open(\"src/text data/Bible_Tagalog.txt\", encoding='utf-8').read()\n",
    "# target_op = open(\"src/text data/Bible_Ilokano.txt\", encoding='utf-8').read()\n",
    "\n",
    "# dict_train = translate_smt_nb(test_doc, target_op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the file\n",
    "test_doc = open(\"src/text data/testing data/Tagalog/tl_test_data_bible.txt\", encoding='utf-8').read()\n",
    "target_op = open(\"src/text data/testing data/Ilokano/il_test_data_bible.txt\", encoding='utf-8').read()\n",
    "\n",
    "dict_test = translate_smt_nb(test_doc, target_op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# dict_op_ex_rec = dict_train.to_dict('records')\n",
    "\n",
    "# try:\n",
    "#     with open(\"src/json data/Tagalog to Ilokano/Hybrid Translator/dict_tl-il_op_ex.json\", \"w\") as outfile:\n",
    "#         json.dump(dict_op_ex_rec, outfile)\n",
    "#     print(\"Successfully saved the file.\")\n",
    "# except:\n",
    "#     print(\"Error in saving the file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Testing result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the file.\n"
     ]
    }
   ],
   "source": [
    "dict_test_rec = dict_test.to_dict('records')\n",
    "\n",
    "try:\n",
    "    with open(\"src/json data/Tagalog to Ilokano/Hybrid Translator/dict_tl-il_test.json\", \"w\") as outfile:\n",
    "        json.dump(dict_test_rec, outfile)\n",
    "    print(\"Successfully saved the file.\")\n",
    "except:\n",
    "    print(\"Error in saving the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scoring'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscoring\u001b[39;00m \u001b[39mimport\u001b[39;00m scoring_bleu, scoring_ter\n\u001b[0;32m      3\u001b[0m ave_bleu \u001b[39m=\u001b[39m scoring_bleu(dict_op_ex)\n\u001b[0;32m      4\u001b[0m ave_ter \u001b[39m=\u001b[39m scoring_ter(dict_op_ex)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scoring'"
     ]
    }
   ],
   "source": [
    "from scoring import scoring_bleu, scoring_ter\n",
    "\n",
    "ave_bleu = scoring_bleu(dict_op_ex)\n",
    "ave_ter = scoring_ter(dict_op_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average TER Score: \", ave_ter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd5e40cb983109c15fc1053f6f3e661cc97e68e07c1758cdbd2441c60186ce19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
